{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TODO :\n",
    "        * Compare the plotted graphs for the interpolated vs daily vs original datasets\n",
    "        * Handle the resampling size for week and month so they use either interpolated or not depending on what I think looks better from the plots (do we even need to do them?)\n",
    "* after doing thh above 2 things should be finished with EDA and preprocessing can move on to feature engineering and data preparation for the RNN algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA of the data\n",
    "* Performing an EDA of the data to discover preliminary patterns and problems which we will use in our preprocessing of the data\n",
    "\n",
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm #lets us display pretty progress bars for loops\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import random #used for randomly selecting which plots to display (as there are a lot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up display options for pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "* Try to load from pickle file as it is quicker to access\n",
    "* If this fails, load the original CSV and save it to a pickle file\n",
    "* <b>No point in doing anything with weather data as we only have 1 years worth of values so it is impossible to leverage with our chosen RNN/CNN based models to predict the next years worth of energy readings</b>\n",
    "    * <b>Unless we leveraged them to produce multivariate predictions and also predicting the weathers values in order to use those as input for subsequent days: but this will likely result in worse performing energy predictions</b>\n",
    "* Hence we are only looking into addInfo and consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#try to load pickle file as it is quicker\n",
    "try:\n",
    "    print(\"Loading consumption via pickle\")\n",
    "    df_energy = pd.read_pickle(\"../Data/Original_Data/consumption.pkl\")\n",
    "# if the file was not found then load the csv then save it to pickle for future loads\n",
    "except:\n",
    "    print(\"Loading consumption via csv\")\n",
    "    df_energy = pd.read_csv(\"../Data/Original_Data/consumption.csv\")\n",
    "    df_energy.to_pickle(\"../Data/Original_Data/consumption.pkl\")\n",
    "\n",
    "    #try to load pickle file as it is quicker\n",
    "try:\n",
    "    print(\"Loading addInfo via pickle\")\n",
    "    df_info = pd.read_pickle(\"../Data/Original_Data/addInfo.pkl\")\n",
    "# if the file was not found then load the csv then save it to pickle for future loads\n",
    "except:\n",
    "    print(\"Loading addInfo via csv\")\n",
    "    df_info = pd.read_csv(\"../Data/Original_Data/addInfo.csv\")\n",
    "    df_info.to_pickle(\"../Data/Original_Data/addInfo.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting df_energy and df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting the values of the dataframe\n",
    "df_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting the shape of the dataframe\n",
    "df_energy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting the values of the dataframe\n",
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting the shape of the dataframe\n",
    "df_info.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions to be drawn\n",
    "\n",
    "* Can see that df_info only has entries for 2140 of the 3248 possible rows\n",
    "* Must add the missing rows filled with NaN\n",
    "    * Done in the EDA on AddInfo section where we simply join based on meter_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many missing values?\n",
    "print(df_energy.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions:\n",
    "* Large number of missing values hence we must find a way to deal with that\n",
    "    * Will experiment with:\n",
    "        1. Imputing the missing values linearly\n",
    "        2. Dropping the missing values entirely and only using values we do have\n",
    "        3. just treating missing values as 0\n",
    "        \n",
    "## Function for resampling into different time periods (will need this a lot)\n",
    "\n",
    "\n",
    "* df_energy = the dataframe we are wanting to reschedule up\n",
    "* file_name = name of file we will save it as\n",
    "* resample_size = \"D\" or \"M\" or \"W\" corresponding to what period we are resampling into\n",
    "* NaN_staregy\n",
    "    * ignore : ignore nans and sum up rows regardless\n",
    "    * removeColumn : if a nan is present in the column corresponding to the new column entry for this row, and set it to nan\n",
    "    * imputeMean : if a nan is present in the columns corresponding to the new column entry for this row, treat them as the mean of the known values and sum up anyway for a reasonable estimate of what the total would have been"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_different_time(df_energy, file_name, resample_size, NaN_strategy):\n",
    "    meter_id=df_energy[\"meter_id\"]\n",
    "    df_energy_new = pd.DataFrame(columns=[\"meter_id\"])\n",
    "    df_energy_new[\"meter_id\"] = meter_id\n",
    "\n",
    "    #for date of the resample size in the range of dates\n",
    "    for new_sample in tqdm(pd.date_range(datetime(2017, 1, 1), datetime(2017, 12, 31), freq = resample_size),position=0):\n",
    "        \n",
    "        #get this columns name as a string\n",
    "        if(resample_size==\"M\"):\n",
    "            columnName = str(new_sample.date())[:7]\n",
    "        elif(resample_size==\"D\" or resample_size==\"W\"):\n",
    "            columnName = str(new_sample.date())\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        #if this isn't a week we can simply take those that start with the new columnName\n",
    "        if(resample_size!=\"W\"):\n",
    "            #get all columns that relate to this new sample\n",
    "            columns = [i for i in df_energy.columns.values[1:] if i.startswith(columnName)]\n",
    "            \n",
    "        #if it is a week be a bit more fancy find the days in the range of this week\n",
    "        else:\n",
    "            columns = []\n",
    "            #for each day in range of this week\n",
    "            for weeks_day in pd.date_range(new_sample, new_sample+timedelta(days=6), freq=\"D\"):\n",
    "                #get todays name as a string\n",
    "                dayName = str(weeks_day.date())\n",
    "                #get all columns that relate to this day\n",
    "                dayColumns = [i for i in df_energy.columns.values[1:] if i.startswith(dayName)]\n",
    "                #add it to the week total\n",
    "                columns = columns+dayColumns\n",
    "                \n",
    "        #if we do not want to accept nans in our new columns\n",
    "        if(NaN_strategy!=\"ignore\"):\n",
    "            #get series saying sum of nans in each row for the columns corresponding to this new column\n",
    "            series_bool_contains_nulls = df_energy[columns].apply(lambda row: row.isnull().sum().sum(), axis=1)\n",
    "            \n",
    "            #for each rows boolean contains nulls truth value\n",
    "            for row_id in tqdm(range(len(series_bool_contains_nulls)), position=1, leave=False):\n",
    "                #this rows corresponding count of nulls\n",
    "                num_contains_null=series_bool_contains_nulls[row_id]\n",
    "                \n",
    "                #if this row contains nulls....\n",
    "                if(num_contains_null>1):\n",
    "                    # if our nan strategy is setting the entire column to nan then do that\n",
    "                    if(NaN_strategy==\"removeColumn\"):\n",
    "                        print(columnName,str(meter_id[row_id]),\"(\",str(row_id),\") contains a null, setting this column to null\")\n",
    "                        df_energy_new.at[row_id,columnName]=np.nan\n",
    "                        \n",
    "                    # if our nan strategy is treating each nan as the groups mean then calculate the sum + add the mean * count of nums\n",
    "                    # effectively imputing the nan entries as the mean to get a reasonable guess of what the total is\n",
    "                    elif(NaN_strategy==\"imputeMean\"):\n",
    "                        rowMean = df_energy[columns].iloc[row_id].mean(skipna=True)\n",
    "                        print(columnName,str(meter_id[row_id]),\"(\",str(row_id),\") contains\",num_contains_null,\"null\\n\",num_contains_null,\"*\",rowMean,\"=\",rowMean*num_contains_null)\n",
    "                        df_energy_new.at[row_id,columnName]=(df_energy[columns].iloc[row_id].sum()+(rowMean * num_contains_null))\n",
    "                    \n",
    "                    # else we aren't using a valid nan strategy so return\n",
    "                    else:\n",
    "                        return\n",
    "                        \n",
    "                #else this row doesn't contain nulls set it's corresponding cell in the new column to the sum\n",
    "                else:\n",
    "                    df_energy_new.at[row_id,columnName]=df_energy[columns].iloc[row_id].sum()\n",
    "\n",
    "        #else we don't care if it contains nans or not so just sum up these columns to our new day    \n",
    "        else:\n",
    "            #sum these up into a value for the new sample size\n",
    "            df_energy_new[columnName] = df_energy[columns].sum(axis=1)\n",
    "    \n",
    "    #if we leave file name blank we don't want to save\n",
    "    if(file_name!=\"\"):\n",
    "        #saving the dataframe\n",
    "        if(os.path.exists(f\"../Data/Preprocessed_Data/{file_name}.pkl\")==False):\n",
    "            df_energy_new.to_pickle(f\"../Data/Preprocessed_Data/{file_name}.pkl\")\n",
    "    return df_energy_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling from 30 min intervals into days \n",
    "* Resmapling into days as it'll be better suited for our RNN based models to learn from\n",
    "    * These are what we will actually predict \n",
    "        * Eg will train on to use the prior 14 time steps (days) to predict the next 7 time steps (days)\n",
    "            * Then will iteratively use those predictions to prediction the next 7 days: until we have done all 365 and then aggregate those predictions into the monthly sums ready for submission\n",
    "            \n",
    "            \n",
    "* TODO : Experiment on if I should accept NaNs or not currently set to True\n",
    "    * Ended up setting to false with daily energy that isn't interpolated will take a hard stance that anything that has missing is considered unreliable and thus will just not consider it at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling the original data into a daily value setting days that contain a missing to be missing\n",
    "df_energy_daily = resample_to_different_time(df_energy, \"consumption_daily_nan_removeColumn\",\"D\",\"removeColumn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_energy_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling the original data into a daily value setting days that ignores missing and still sums the day \n",
    "df_energy_daily_with_missing = resample_to_different_time(df_energy, \"consumption_daily_nan_ignored\",\"D\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy_daily_with_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly interpolating the missing values from 30 min then resampling into days\n",
    "\n",
    "* Linearly interpolate the half hours with a limit of 4 hours worth of consecutive missing values\n",
    "* Then aggregate into days setting days containing a missing to the entire day as missing\n",
    "* Then interpolating those missing days\n",
    "\n",
    "- Experiment with different limits for how many consecutive nans are okay to fit \n",
    "    - current set to 8 (4 hours worth)\n",
    "    \n",
    "* Experiment with using acceptNaNs and not\n",
    "    \n",
    "* TODO : Experiment on if I should accept NaNs or not. Currently set to false then interpolating the days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many consecutive NaNs to fill\n",
    "seq_nan_limit = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dropping the meter id column for now so we can interpolate\n",
    "meter_id = df_energy[\"meter_id\"]\n",
    "df_energy_interpolate = df_energy.drop([\"meter_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing 0 back to NaN\n",
    "df_energy_interpolate=df_energy_interpolate.replace(0,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolating the missing values\n",
    "df_energy_interpolate = df_energy_interpolate.interpolate(method=\"linear\", axis=1, limit = 2) \n",
    "# adding the meter id back on\n",
    "df_energy_interpolate = pd.concat([meter_id,df_energy_interpolate],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy_interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#resampling to days\n",
    "df_energy_interpolate_days = resample_to_different_time(df_energy_interpolate, \"\",\"D\",\"removeColumn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy_interpolate_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing 0 with nan\n",
    "df_energy_interpolate_days_interpolate = df_energy_interpolate_days.replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping meter_id\n",
    "meter_id = df_energy_interpolate_days_interpolate[\"meter_id\"] \n",
    "df_energy_interpolate_days_interpolate=df_energy_interpolate_days_interpolate.drop(\"meter_id\", axis=1)\n",
    "#interpolating the days we set to nan due to containing a missing value as acceptNaNs is false\n",
    "df_energy_interpolate_days_interpolate = df_energy_interpolate_days_interpolate.interpolate(method=\"linear\", axis=1) \n",
    "#adding meter_id back on\n",
    "df_energy_interpolate_days_interpolate = pd.concat([pd.DataFrame(meter_id), df_energy_interpolate_days_interpolate],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_energy_interpolate_days_interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe\n",
    "if(os.path.exists(\"../Data/Preprocessed_Data/consumption_daily_nan_interpolated.pkl\")==False):\n",
    "        df_energy_interpolate_days_interpolate.to_pickle(\"../Data/Preprocessed_Data/consumption_daily_nan_interpolated.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting graph of the time patterns for each meter\n",
    "\n",
    "time_step_size =\n",
    "* O : Original\n",
    "* D : Day\n",
    "* M : Month\n",
    "* W : Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for plotting the day, week, and month readings of each meter on top of each other\n",
    "def plot_time_patterns(df_energy_day, directory, show_pid, time_step_size):\n",
    "      \n",
    "    df_energy_day_plot = df_energy_day.drop([\"meter_id\"], axis=1)\n",
    "   \n",
    "    for pid in tqdm(range(0,3248)):\n",
    "        \n",
    "        #getting this row\n",
    "        meter_id = df_energy_day.iloc[pid,0]\n",
    "        df_days_pid = df_energy_day_plot[pid:pid+1].T\n",
    "\n",
    "        #converting index to datetime for ease of plots key\n",
    "        df_days_pid.index=pd.to_datetime(df_days_pid.index)\n",
    "        \n",
    "        #creating figure\n",
    "        fig = plt.figure(figsize=(20,10))\n",
    "        \n",
    "        # if this is already in days just plot the days with the weekly and monthly moving averages\n",
    "        if(time_step_size == \"D\"):\n",
    "            #calculating the week and month using moving averages\n",
    "            week_average=df_days_pid.rolling(7).mean()\n",
    "            month_average=df_days_pid.rolling(30).mean()\n",
    "            #plot the weekly and monthly moving averages\n",
    "            plt.plot(week_average, label=\"weekly moving average\", lw=1.5, color=\"coral\")\n",
    "            plt.plot(month_average, label=\"monthly moving average\", lw=2, color=\"royalblue\")\n",
    "            plt.plot(df_days_pid, label=\"total daily energy\", lw=1,color=\"olivedrab\")\n",
    "        \n",
    "        #elif this is the original half an hour readings also add a separate moving average for the day\n",
    "        elif(time_step_size == \"O\"):\n",
    "            #plotting half hourly energy\n",
    "            plt.plot(df_days_pid, label=\"total half hourly energy\", lw=0.3,color=\"pink\")\n",
    "        \n",
    "        #elif this the weeks simply plot the weekly values\n",
    "        elif(time_step_size == \"W\"):\n",
    "            #plotting half hourly energy\n",
    "            plt.plot(df_days_pid, label=\"total weekly energy\", lw=1,color=\"coral\")\n",
    "            \n",
    "        #elif this the month simply plot the monthly values\n",
    "        elif(time_step_size == \"M\"):\n",
    "            #plotting half hourly energy\n",
    "            plt.plot(df_days_pid, label=\"total monthly energy\", lw=1,color=\"royalblue\")\n",
    "                     \n",
    "        #else it's uncrecognised so return\n",
    "        else:\n",
    "            return\n",
    "            \n",
    "        #annotations\n",
    "        plt.legend(fontsize=20)\n",
    "        plt.title(\"Meter '\"+str(meter_id)+\"' energy readings\", fontsize=15)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Energy Usage kWh\")\n",
    "        \n",
    "        #showing a subset of the plots\n",
    "        if(pid in show_pid):\n",
    "            print(\"showing row: \",pid)\n",
    "            plt.show()\n",
    "        \n",
    "        #saving plot if it doesn't already exist\n",
    "        if(os.path.exists(f\"../EDA/plots/consumption/{directory}/energy_usage_{pid}_{meter_id}.png\")==False):\n",
    "            fig.savefig(f\"../EDA/plots/consumption/{directory}/energy_usage_{pid}_{meter_id}.png\")\n",
    "        \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting plot id's we want to show (random amount as no point in showing 9000 plots in compiler it'll make it unnecessarily slow)\n",
    "#display_pid = random.sample(range(0,3248),showPlots) # new randomly selected sample\n",
    "#display_pid = [651, 3034, 1346, 1618, 2105] # previously randomly selected sample which we can repeat\n",
    "\n",
    "display_pid=[849,2410,2676,2680,2965,3158]#meters which contained values from within the max sequential nan limit\n",
    "display_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotting the original data\n",
    "plot_time_patterns(df_energy.replace(np.nan,0), \"Original\", display_pid, \"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the uninterpolated data with days with missing still summed to a daily value (but inaccurate as some hours missing)\n",
    "plot_time_patterns(df_energy_daily_with_missing.replace(np.nan, 0), \"Daily_Ignored\", display_pid, \"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotting the uninterpolated data with days with missing set to missing\n",
    "plot_time_patterns(df_energy_daily.replace(np.nan, 0), \"Daily_Removed\", display_pid, \"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotting the interpolated data\n",
    "plot_time_patterns(df_energy_interpolate_days_interpolate.replace(np.nan,0), \"Daily_Interpolated\", display_pid, \"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing how many missing values are now missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Interpolated dataset has this many missing values\")\n",
    "print(df_energy_interpolate_days_interpolate.replace(0, np.nan).isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uninterpolated dataset with days with missing values set to missing has this many missing values\")\n",
    "print(df_energy_daily.replace(0, np.nan).isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uninterpolated dataset with days with missing values set to sum of values it does have has this many missing values\")\n",
    "print(df_energy_daily_with_missing.replace(0, np.nan).isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Essentially can conclude interpolated has successfully done a good job at imputing the missing values\n",
    "    * Has similar pattern to that of the summed with missing\n",
    "        * But more accurate and much fewer missing values\n",
    "* Will use this dataset in the future for that reason\n",
    "* Now only has the 'prefix nans' still missing (nans from before the meters started)\n",
    "    * And some 'sufix na' (when meter failed and stopped recording\n",
    "        * Will need more EDA to find where these are and handle them\n",
    "            * Perhaps just write a script checking % variance and if it's at unexpected level then it's possibly failed?\n",
    "                * Eg this value is 99% lower than the yearly average (probably means something went wrong)v\n",
    "\n",
    "## Resampling into different sizes for different prediction scales we will experiment with\n",
    "* Going to experiment with framing this data into different time series problems and assessing which problems our model performs best at\n",
    "    * E.g. predicting the next day (already processed data into days)\n",
    "    * E.g. predicting the next week\n",
    "    * E.g. predicting the next month\n",
    "    * E.g. predicting the next year\n",
    "    \n",
    "* Way I am thinking of using the models is training in the days as timesteps then experimenting on testing it's ability for predicting the different time scales\n",
    "    * For predicting next day and next week can probably get by on just using the data we have and splitting it into a test and train\n",
    "    * For predicting next month and next year will have to submit to competition as we only have 12 months worth of data and the test set is on the comp server and inaccessible\n",
    "    \n",
    "* Hence for predicting smaller periods we will be able to train our RNN to make multistep predictions as we have sufficient data for that\n",
    "* However, we don't have enough to predict next 12 months so will have to train it to predict a smaller amount and then iteratively use that prediction as another input to predict the next step\n",
    "\n",
    "### Resampling the daily values into weekly values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resampling data into weeks\n",
    "df_energy_weekly_interpolate = resample_to_different_time(df_energy_interpolate_days_interpolate, \"consumption_weekly_interpolated_removed\",\"W\",\"removeColumn\").iloc[:, :-1] # dropping last 'week' as it's only 1 day\n",
    "df_energy_weekly_interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling the daily values into monthly values\n",
    "Acceptings NaNs as otherwise we get some rows without "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resampling daily data into months\n",
    "df_energy_monthly_interpolate = resample_to_different_time(df_energy_interpolate_days_interpolate, \"consumption_monthly_interpolated_imputedMean\",\"M\",\"imputeMean\") # treating missing days as the mean as it's a reasonable estimate\n",
    "df_energy_monthly_interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the weekly and monthly values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the weekly data\n",
    "plot_time_patterns(df_energy_weekly_interpolate.replace(np.nan,0), \"Interpolated_Weekly_Removed\", display_pid, \"W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the monthly data\n",
    "plot_time_patterns(df_energy_monthly_interpolate.replace(np.nan,0), \"Interpolated_Monthly_ImputedMean\", display_pid, \"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating monthly averages (not sure if needed or not so commented out; may be worth using as another column after preparing data for time series?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#resampling into months\n",
    "df_energy_month = pd.DataFrame(columns=[\"meter_id\"])\n",
    "df_energy_month[\"meter_id\"] = meter_id\n",
    "\n",
    "#for each month in the range of dates\n",
    "for month_i in tqdm(pd.date_range(datetime(2017, 1, 1), datetime(2017, 12, 31), freq = \"M\")):\n",
    "    #going to give column name middle of month temporarily for ease of plotting, then will remove\n",
    "    middle_of_month = str(month_i.replace(day=1)+(month_i-month_i.replace(day=1))/2)\n",
    "    \n",
    "    #get this months name as a string\n",
    "    monthName = str(month_i.date())[:7]\n",
    "    #get all columns that relate to this months\n",
    "    col_month_i = [i for i in df_energy_daily.columns.values[1:] if i.startswith(monthName)]\n",
    "    #sum these up into a monthly value\n",
    "    df_energy_month[middle_of_month] = df_energy_daily[col_month_i].mean(axis=1) # midway through month so easier to plot\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions:\n",
    "* Have removed the missing values that could be removed via linear interpolation:\n",
    "    * when there are values before and after the sequence of ?\n",
    "* Still have lots of missing values at start and end of dataset\n",
    "    * IN the data preparation for RNN and feature engineering stage will add a way to deal with this as mentioned perhaps checking if it falls under a certain threshold and then classifying the meter as not working that day if it was under that threshold and then set the day as missing\n",
    "        * As it'll be atypical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_________\n",
    "_________________________________________________________\n",
    "_________\n",
    "\n",
    "\n",
    "# EDA on AddInfo\n",
    "## Adding missing rows to AddInfo based on missing meter_id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first making both dataframes alphabetical in order of meter_id for ease of comparison\n",
    "df_info.sort_values('meter_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy.sort_values('meter_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the missing meter rows to df_info\n",
    "df_info_all_meters = pd.merge(df_info, df_energy[\"meter_id\"], on=\"meter_id\",how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the values are correct now\n",
    "df_info_all_meters.sort_values('meter_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape is correct now\n",
    "df_info_all_meters.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring datatypes and unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspecting the types of each column\n",
    "df_info_all_meters.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#looking at the unique values for each column\n",
    "for column in df_info_all_meters.drop([\"meter_id\"],axis=1).columns:\n",
    "    print(column+\" unique value count: \"+str(df_info_all_meters[column].nunique()))\n",
    "    print(df_info_all_meters[column].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We now know:\n",
    "    1. The columns that contain categorical values vs the columns that contain real numbers\n",
    "    2. The unique values for each column\n",
    "    \n",
    "## Exploring missing values\n",
    "### Visualising as histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting a histogram for each of the columns in the dataset\n",
    "for column in df_info_all_meters.drop([\"meter_id\"],axis=1).columns:\n",
    "    df_temp = df_info_all_meters[column].replace(np.nan,\"NaN\").apply(str)\n",
    "    ax = df_temp.hist(figsize=(10,10))\n",
    "    ax.set_xlabel(\"unique values\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "    fig = ax.get_figure()\n",
    "    plt.title(column+\" values\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    if(os.path.exists(f\"../EDA/plots/addInfo/{column}_hist.png\")==False):\n",
    "        fig.savefig(f\"../EDA/plots/addInfo/{column}_hist.png\")\n",
    "        \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating percentage of missing values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating percentage of missing values\n",
    "#stores index of columns to keep: defaults all to true and will check which to make false\n",
    "columns = np.full((df_info_all_meters.shape[1],), True, dtype=bool)\n",
    "\n",
    "# getting the count of each object type in each column\n",
    "for i in range(0,len(df_info_all_meters.columns)):\n",
    "    column =df_info_all_meters.columns[i]\n",
    "    #calculating the percentage of missing values\n",
    "    percent_missing = df_info_all_meters[column].isnull().sum() * 100 / len(df_info_all_meters)\n",
    "    print(column+\" contains missing values: \"+str(df_info_all_meters[column].isna().values.any()))\n",
    "    print(column+\" is missing: \"+str(round(percent_missing,2))+\"%\")\n",
    "    \n",
    "    #if the percentage of missing values is greater than 50% mark them to be removed\n",
    "    if(percent_missing>=50):\n",
    "        print(\"Marked \",column,\"for removal due to it having more than 50% missing values\")\n",
    "        columns[i] = False\n",
    "    else:\n",
    "        print(\"Not marked \",column,\"for removal due to it not having more than 50% missing values\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meaning of findings;\n",
    "* As the histograms and calculations show: there is a massive amount of missing values\n",
    "* Marked all columns with > 50% missing values for removal\n",
    "    * as if more than half of the values are missing it is likely what we do impute won't be accurate\n",
    "* Need to decide what to do with the the remaining 2 as they have 40% missing values: is this good enough?\n",
    "    * Will likely require experimentation:\n",
    "        * So will run one model which only uses energy\n",
    "        * And one that uses addInfo after the missing values have been imputed\n",
    "\n",
    "## Removing the columns with greater than 50% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the columns marked for removal\n",
    "selected_columns = df_info_all_meters.columns[columns]\n",
    "df_info_all_meters = df_info_all_meters[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting the current state of df_info_all_meters\n",
    "df_info_all_meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing the missing values\n",
    "* May be worth experimenting with different imputation methods\n",
    "* But for now just going with KNN\n",
    "\n",
    "### Preparing dataframe for use by KNNImputer\n",
    "### Adding information on the energy time series data that may be useful for imputing the missing addInfo values\n",
    "Experimenting with different combinations 9think it may have been too many when I originally used 4 different energy ones and it got weighted too highly skewing it due to the multi collinearity problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the total, mean, max, and min energy usage only using KNOWN values (not including the interpolated ones)\n",
    "df_energy_info = pd.DataFrame(columns=[\"meter_id\",\n",
    "                                       \"mean_energy_usage\",\n",
    "                                       #\"max_energy_usage\",\n",
    "                                       #\"min_energy_usage\",\n",
    "                                       \"total_energy_usage\"\n",
    "                                      ])\n",
    "df_energy_info[\"meter_id\"]=df_energy_interpolate_days_interpolate[\"meter_id\"].values\n",
    "df_energy_info[\"mean_energy_usage\"] = df_energy_interpolate_days_interpolate.replace(0,np.nan).iloc[:, 1:].mean(axis=1).round(3)\n",
    "#df_energy_info[\"max_energy_usage\"] = df_energy_interpolate_days_interpolate.replace(0,np.nan).iloc[:, 1:].max(axis=1).round(3)\n",
    "#df_energy_info[\"min_energy_usage\"] = df_energy_interpolate_days_interpolate.replace(0,np.nan).iloc[:, 1:].min(axis=1).round(3)\n",
    "df_energy_info[\"total_energy_usage\"] = df_energy_interpolate_days_interpolate.replace(0,np.nan).iloc[:, 1:].sum(axis=1).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy_info#.loc[df_energy_means[\"meter_id\"]==\"0xa62b9f23553ff183f61e2bf943aab3d5983d02d7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the total, mean, max, and min to the df for use with knn imputation\n",
    "df_info_knn = df_info_all_meters\n",
    "df_info_knn = df_info_knn.merge(df_energy_info[[\"meter_id\",\n",
    "                                                \"mean_energy_usage\",\n",
    "                                                #\"max_energy_usage\",\n",
    "                                                #\"min_energy_usage\", \n",
    "                                                \"total_energy_usage\"\n",
    "                                               ]], on=\"meter_id\",how=\"left\")\n",
    "df_info_knn#.loc[df_info_knn2[\"meter_id\"]==\"0xa62b9f23553ff183f61e2bf943aab3d5983d02d7\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping meter_id, encoding and normalizing data so it is suitable for KNN Imputation\n",
    "#### Dropping meter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the id column so it doesn't negatively influence the knn algorithm\n",
    "meter_id = df_info_knn[\"meter_id\"]\n",
    "df_info_knn = df_info_knn.drop([\"meter_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting the df without the id column\n",
    "df_info_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the categorical values ready for knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_knn_encoded = df_info_knn\n",
    "df_info_knn_encoded[\"dwelling_type\"]=df_info_knn_encoded[\"dwelling_type\"].replace(\"flat\",0).replace(\"terraced_house\",1).replace(\"semi_detached_house\",2).replace(\"detached_house\",3).replace(\"bungalow\",4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_knn_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data as KNN is done via a distance measure and we don't want to skew it\n",
    "scaler = MinMaxScaler()\n",
    "df_info_knn_normalized = pd.DataFrame(scaler.fit_transform(df_info_knn_encoded), columns = df_info_knn_encoded.columns)\n",
    "\n",
    "# inspecting new state of the dataframe after normalisation\n",
    "df_info_knn_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now data is prepared, using KNN to impute the missing values\n",
    "* Experiment with different numbers for nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using KNN to impute the missing values\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_info_knn_imputed = pd.DataFrame(imputer.fit_transform(df_info_knn_normalized),columns = df_info_knn_normalized.columns)\n",
    "\n",
    "# inspecting new state of the dataset now it no longer has missing values\n",
    "df_info_knn_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking it no longer has missing values\n",
    "df_info_knn_imputed.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at sum of missing values\n",
    "df_info_knn_imputed.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning these imputed values to usable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unnormalsing the data\n",
    "df_info_knn_imputed.columns[1:] #ignoring meter id as that wasn't normalized\n",
    "df_info_knn_finished = pd.DataFrame(scaler.inverse_transform(df_info_knn_imputed), columns = df_info_knn_imputed.columns) # reversing the normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the unormalised data\n",
    "df_info_knn_finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rounding bedrooms to their final value\n",
    "num_bedrooms=df_info_knn_finished[\"num_bedrooms\"].round() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rounding dwelling type to a real value\n",
    "df_dwellings=df_info_knn_finished[\"dwelling_type\"].round() \n",
    "#reverting the encoding\n",
    "df_dwellings=df_dwellings.replace(0, \"flat\").replace(1, \"terraced_house\").replace(2, \"semi_detached_house\").replace(3, \"detached_house\").replace(4, \"bungalow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating the meter_id, number of bedrooms, and dwellings columns\n",
    "df_info_knn_finished = pd.concat([meter_id, num_bedrooms, df_dwellings], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting the final configuration of the knn imputed values\n",
    "df_info_knn_finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the unique values are the same as the original set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the unique values for each column before imputation\n",
    "print(\"Unique values before imputation\")\n",
    "\n",
    "print(\"num_bedrooms unique value count: \"+str(df_info_all_meters[\"num_bedrooms\"].nunique()))\n",
    "print(df_info_all_meters[\"num_bedrooms\"].unique())\n",
    "print()\n",
    "print(\"dwelling_type unique value count: \"+str(df_info_all_meters[\"dwelling_type\"].nunique()))\n",
    "print(df_info_all_meters[\"dwelling_type\"].unique())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the unique values for each column before imputation\n",
    "print(\"Unique values after imputation\\n\")\n",
    "\n",
    "print(\"num_bedrooms unique value count: \"+str(df_info_knn_finished[\"num_bedrooms\"].nunique()))\n",
    "print(df_info_knn_finished[\"num_bedrooms\"].unique())\n",
    "print()\n",
    "print(\"dwelling_type unique value count: \"+str(df_info_knn_finished[\"dwelling_type\"].nunique()))\n",
    "print(df_info_knn_finished[\"dwelling_type\"].unique())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the distribution of unique values that were distributed to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at percentage distribution of values before imputation\n",
    "print(\"Distribution of unique values before imputation\")\n",
    "print()\n",
    "\n",
    "num_bedrooms_percentages = df_info_all_meters['num_bedrooms'].value_counts(normalize=True) * 100\n",
    "print(\"num_bedrooms percentages\")\n",
    "print(num_bedrooms_percentages)\n",
    "print()\n",
    "print(\"num_bedrooms totals\")\n",
    "print(df_info_all_meters['num_bedrooms'].value_counts())\n",
    "print()\n",
    "print()\n",
    "\n",
    "dwelling_type_percentages = df_info_all_meters['dwelling_type'].value_counts(normalize=True) * 100\n",
    "print(\"dwelling_type percentages\")\n",
    "print(dwelling_type_percentages)\n",
    "print()\n",
    "print(\"dwelling_type totals\")\n",
    "print(df_info_all_meters['dwelling_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at percentage distribution of values after imputation\n",
    "print(\"Distribution of unique values before imputation\")\n",
    "print()\n",
    "\n",
    "num_bedrooms_percentages = df_info_knn_finished['num_bedrooms'].value_counts(normalize=True) * 100\n",
    "print(\"num_bedrooms percentages\")\n",
    "print(num_bedrooms_percentages)\n",
    "print()\n",
    "print(\"num_bedrooms totals\")\n",
    "print(df_info_knn_finished['num_bedrooms'].value_counts())\n",
    "print()\n",
    "print()\n",
    "\n",
    "dwelling_type_percentages = df_info_knn_finished['dwelling_type'].value_counts(normalize=True) * 100\n",
    "print(\"dwelling_type percentages\")\n",
    "print(dwelling_type_percentages)\n",
    "print()\n",
    "print(\"dwelling_type totals\")\n",
    "print(df_info_knn_finished['dwelling_type'].value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions:\n",
    "* Hasn't imputed any meters to be a flat\n",
    "* Hasn't imputed any meters to have 1 or 5 bedrooms\n",
    "* Has essentially increased the % divide\n",
    "    * Higher % represented classes were more likely to be picked\n",
    "    * Whilst lower % represented classes were less likely to be picked \n",
    "* Expected behaviour but is this useful and has it imputed correctly?\n",
    "* As mentioned already will experiment and see if using these meters or not improves performance more\n",
    "    * From this we can infer if this was effective or not\n",
    "\n",
    "## Plotting hists of the imputed addInfo\n",
    "plotting hists of the imputed addInfo columns before and after imputation to compare their patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a histogram for each of the imputed columns in the dataset before imputation\n",
    "for column in df_info_knn_finished.drop([\"meter_id\"],axis=1).columns:\n",
    "    df_temp = df_info_all_meters[column]\n",
    "    ax = df_temp.hist(figsize=(10,10))\n",
    "    ax.set_xlabel(\"unique values\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "    fig = ax.get_figure()\n",
    "    plt.title(column+\" values\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    if(os.path.exists(f\"../EDA/plots/addInfo/Pre-Imputation/{column}_preimp_hist.png\")==False):\n",
    "        fig.savefig(f\"../EDA/plots/addInfo/Pre-Imputation/{column}_preimp_hist.png\")\n",
    "        \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting a histogram for each of the columns in the dataset after imputation\n",
    "for column in df_info_knn_finished.drop([\"meter_id\"],axis=1).columns:\n",
    "    df_temp = df_info_knn_finished[column]\n",
    "    ax = df_temp.hist(figsize=(10,10))\n",
    "    ax.set_xlabel(\"unique values\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "    fig = ax.get_figure()\n",
    "    plt.title(column+\" values\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    if(os.path.exists(f\"../EDA/plots/addInfo/Imputed/{column}_imp_hist.png\")==False):\n",
    "        fig.savefig(f\"../EDA/plots/addInfo/Imputed/{column}_imp_hist.png\")\n",
    "        \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneHot Encoding Ready for the RNN to make use of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the categorical values via one hot encoding\n",
    "cat_variables = df_info_knn_finished[df_info_knn_finished.loc[:,df_info_knn_finished.columns!=\"meter_id\"].select_dtypes(include=['object']).columns] # getting the categorical columns (just 'dwelling_type')\n",
    "cat_dummies = pd.get_dummies(cat_variables, drop_first=True, dummy_na=True) # getting dummies, including a separate one for na\n",
    "\n",
    "#setting nan rows and droppoing nan column\n",
    "cat_dummies.loc[cat_dummies.dwelling_type_nan==1, [\"dwelling_type_detached_house\",\"dwelling_type_flat\",\"dwelling_type_semi_detached_house\",\"dwelling_type_terraced_house\"]]=np.nan\n",
    "cat_dummies=cat_dummies.drop(\"dwelling_type_nan\",axis=1)\n",
    "cat_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dropping those original columns then adding the dummy values\n",
    "df_info_knn_finished = df_info_knn_finished.drop(df_info_knn_finished.loc[:,df_info_knn_finished.columns!=\"meter_id\"].select_dtypes(include=['object']).columns, axis=1)\n",
    "df_info_knn_finished = pd.concat([df_info_knn_finished, cat_dummies], axis=1)\n",
    "# inspecting new state of the dataframe with the dummy variables\n",
    "df_info_knn_finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating correlation plot of the features to make sure there is minimal redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the correlation matrix\n",
    "corr_info = df_info_knn_finished.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the correlation heat map\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr_info, cmap=\"coolwarm\", annot=True, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each feature on the x axis\n",
    "for i in range(corr_info.shape[0]):\n",
    "    # for each feature after i\n",
    "    for j in range(i+1, corr_info.shape[0]):\n",
    "        #check if their correlation is above a threshold % (50%)\n",
    "        if abs(corr_info.iloc[i,j]) >= 0.5:\n",
    "            print(\"\\nConsider \",corr_info.columns[j],\" for removal as it has a high correlation with \",corr_info.columns[i],\" of\",round(corr_info.iloc[i,j],2),\"\\n\")\n",
    "            #set column j to be removed \n",
    "            if columns[j]:\n",
    "                columns[j+1] = False #+1 for meter id\n",
    "        else: \n",
    "            print(\"Keeping \",corr_info.columns[i],\" and \",corr_info.columns[j],\" have a correlation of \", round(corr_info.iloc[i,j],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features don't have a particularly high correlation; no need to remove them based upon this\n",
    "\n",
    "## Saving the finished AddInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.exists(f\"../Data/Preprocessed_Data/addInfo_knn.pkl\")==False):\n",
    "        df_info_knn_finished.to_pickle(f\"../Data/Preprocessed_Data/addInfo_knn.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle_tf",
   "language": "python",
   "name": "mle_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
